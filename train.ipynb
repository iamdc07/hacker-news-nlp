{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to train: 352.945272\n",
      "\n",
      "precision: story -  0.9933787838326081  | ask_hn -  0.9487179487179487  | show_hn -  0.9636198271941792  | poll -  0.0\n",
      "recall: story -  0.997028032667991  | ask_hn -  0.9565456545654566  | show_hn -  0.8643687538241893  | poll -  0.0\n",
      "F1 Measure: story -  0.995200062949994  | ask_hn -  0.9526157217200767  | show_hn -  0.9112998602300828  | poll -  0.0\n",
      "\n",
      "Accuracy: 0.990635134642714\n",
      "Precision: 0.990496820258534\n",
      "Recall: 0.990635134642714\n",
      "F1 Measure: 0.9904659676150714\n",
      "Confusion Matrix:\n",
      " [[     0      0      1      5]\n",
      " [     0   4238     61    604]\n",
      " [     0      3   5217    234]\n",
      " [     0    157    220 126475]]\n",
      "TOTAL TIME TAKEN IN (S): 535.8863309999999\n",
      "TOTAL TIME TAKEN IN (MINUTES): 8.93143885\n",
      "-------------------------------------------------\n",
      "Choose your experiment\n",
      "2. Stopwords\n",
      "3. Word length Filtering\n",
      "4. Infrequent Word Filtering\n",
      "5. Smoothing\n",
      "\n",
      "Type '-1' to exit\n",
      "Enter your choice:2\n",
      "\n",
      "Time to train: 311.2491560000001\n",
      "\n",
      "precision: story -  0.9959585614684681  | ask_hn -  0.9490582959641256  | show_hn -  0.9589798087141339  | poll -  0.0\n",
      "recall: story -  0.9966102229369659  | ask_hn -  0.9701136780344701  | show_hn -  0.9202529063838466  | poll -  0.0\n",
      "F1 Measure: story -  0.9962842856411085  | ask_hn -  0.9594704868981775  | show_hn -  0.939217318900916  | poll -  0.0\n",
      "\n",
      "Accuracy: 0.9927850453667602\n",
      "Precision: 0.9927294930416636\n",
      "Recall: 0.9927850453667602\n",
      "F1 Measure: 0.9927383212641455\n",
      "Confusion Matrix:\n",
      " [[     0      0      0      6]\n",
      " [     0   4512     44    347]\n",
      " [     0      3   5291    160]\n",
      " [     0    190    240 126422]]\n",
      "TOTAL TIME TAKEN IN (S): 487.63363200000015\n",
      "TOTAL TIME TAKEN IN (MINUTES): 8.127227200000002\n",
      "-------------------------------------------------\n",
      "Choose your experiment\n",
      "2. Stopwords\n",
      "3. Word length Filtering\n",
      "4. Infrequent Word Filtering\n",
      "5. Smoothing\n",
      "\n",
      "Type '-1' to exit\n",
      "Enter your choice:3\n",
      "\n",
      "Time to train: 289.9452020000001\n",
      "\n",
      "precision: story -  0.9673452532081186  | ask_hn -  0.9489194499017681  | show_hn -  0.9361612767744645  | poll -  0.0\n",
      "recall: story -  0.997162047110018  | ask_hn -  0.7084708470847084  | show_hn -  0.4546196206404242  | poll -  0.0\n",
      "F1 Measure: story -  0.9820273742886645  | ask_hn -  0.8112534117153053  | show_hn -  0.6120263591433278  | poll -  0.0\n",
      "\n",
      "Accuracy: 0.9662573333819189\n",
      "Precision: 0.9654562947181117\n",
      "Recall: 0.9662573333819189\n",
      "F1 Measure: 0.9619755699423581\n",
      "Confusion Matrix:\n",
      " [[     0      0      0      6]\n",
      " [     0   2229      0   2674]\n",
      " [     0      0   3864   1590]\n",
      " [     0    152    208 126492]]\n",
      "TOTAL TIME TAKEN IN (S): 445.688762\n",
      "TOTAL TIME TAKEN IN (MINUTES): 7.428146033333333\n",
      "-------------------------------------------------\n",
      "Choose your experiment\n",
      "2. Stopwords\n",
      "3. Word length Filtering\n",
      "4. Infrequent Word Filtering\n",
      "5. Smoothing\n",
      "\n",
      "Type '-1' to exit\n",
      "Enter your choice:-1\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Intro to AI: Project 2\n",
    "# Written by Dhaval Chavada(40078885) & Anand Kacha (40047673)\n",
    "# For COMP 6721 (Lab section - FK) - Fall 2019\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import experiments\n",
    "import time\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "remove_freq = 1\n",
    "remove_percent = 0\n",
    "smoothing_value = 0\n",
    "\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def read_file(exp=1):\n",
    "    global df_testing\n",
    "    global df_training\n",
    "\n",
    "    df = pd.read_csv(\"./hn2018_2019.csv\")\n",
    "    # df = pd.read_csv(\"./sample.csv\")\n",
    "    df = df.drop(columns=df.columns[0])\n",
    "    df['date'] = pd.to_datetime(df['Created At'])\n",
    "    start_date = '2018-01-01 00:00:00'\n",
    "    end_date = '2018-12-31 00:00:00'\n",
    "    mask_2018 = (df['date'] > start_date) & (df['date'] <= end_date)\n",
    "    start_date = '2019-01-01 00:00:00'\n",
    "    end_date = '2019-12-31 00:00:00'\n",
    "    mask_2019 = (df['date'] > start_date) & (df['date'] <= end_date)\n",
    "    df_training = df.loc[mask_2018]\n",
    "    df_testing = df.loc[mask_2019]\n",
    "\n",
    "    build_vocabulary(df_training, exp)\n",
    "\n",
    "\n",
    "def build_vocabulary(df, exp):\n",
    "    global word_freq_dict\n",
    "    global start_time\n",
    "    global words_removed\n",
    "    word_freq_dict = {}\n",
    "    words_removed = set()\n",
    "\n",
    "    start_time = time.process_time()\n",
    "\n",
    "    if exp == 2:\n",
    "        stop_words_df = pd.read_csv(\"./Stopwords.txt\")\n",
    "        stop_words = stop_words_df[\"a\"].tolist()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\", False, True)\n",
    "\n",
    "        raw = tokenizer.tokenize(row[\"Title\"].lower())\n",
    "\n",
    "        temp1 = tokenizer.tokenize(row[\"Title\"])\n",
    "\n",
    "        title = ' '.join(temp1)\n",
    "\n",
    "        if exp == 2:\n",
    "            raw = list(set(raw).difference(stop_words))\n",
    "            row[\"Title\"] = ' '.join([str(elem) for elem in raw])\n",
    "        elif exp == 3:\n",
    "            for each in raw:\n",
    "                if len(each) >= 9 or len(each) <= 2:\n",
    "                    raw.remove(each)\n",
    "\n",
    "        tokenize_word(raw, title, df, index, words_removed)\n",
    "\n",
    "    od = OrderedDict(sorted(word_freq_dict.items()))\n",
    "\n",
    "    with open('frequency_dict.txt', 'w') as file:\n",
    "        for key, val in od.items():\n",
    "            file.write(str(key) + \" \" + str(val) + \"\\n\")\n",
    "\n",
    "    with open(\"./remove_word.txt\", \"w\") as file:\n",
    "        for element in words_removed:\n",
    "            file.write(element + \"\\n\")\n",
    "\n",
    "    train(od, exp)\n",
    "\n",
    "    total_time = time.process_time() - start_time\n",
    "    print('TOTAL TIME TAKEN IN (S):', total_time)\n",
    "    print('TOTAL TIME TAKEN IN (MINUTES):', total_time / 60)\n",
    "    print(\"-------------------------------------------------\")\n",
    "\n",
    "\n",
    "def tokenize_word(raw, title, df, index, w_removed, testing=False):\n",
    "    bigrams = []\n",
    "    word_list = []\n",
    "\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "    bigrm = list(nltk.bigrams(title.split()))\n",
    "    pos = nltk.pos_tag(raw)\n",
    "    pos_dict = dict(pos)\n",
    "\n",
    "    for i in bigrm:\n",
    "        bigrams.append((''.join([w + ' ' for w in i])).strip())\n",
    "\n",
    "    for each_element in bigrams:\n",
    "        word = each_element.split(' ')\n",
    "\n",
    "        indices_0 = [i for i, e in enumerate(raw) if e == word[0].lower()]\n",
    "        if len(indices_0) != 0:\n",
    "            indices_1 = [i for i, e in enumerate(raw[indices_0[0] + 1:]) if e == word[1].lower()]\n",
    "        else:\n",
    "            indices_1 = [i for i, e in enumerate(raw) if e == word[1].lower()]\n",
    "\n",
    "        if word[0].istitle() and word[1].istitle():\n",
    "            if len(indices_0) > 0 and (\n",
    "                    pos_dict.get(word[0].lower()) == 'NN' or pos_dict.get(word[0].lower()) == 'NNS'):\n",
    "                if len(indices_1) > 0 and (\n",
    "                        pos_dict.get(word[1].lower()) == 'NN' or pos_dict.get(word[1].lower()) == 'NNS'):\n",
    "                    raw.remove(word[0].lower())\n",
    "                    # raw.index(word[1].lower())\n",
    "                    raw.remove(word[1].lower())\n",
    "\n",
    "                    if testing is False:\n",
    "                        temp = each_element.lower() + \"-\" + df.at[index, 'Post Type']\n",
    "                        freq = word_freq_dict.get(temp)\n",
    "                        raw.append(each_element.lower())\n",
    "                        if freq is None:\n",
    "                            word_freq_dict[temp] = 1\n",
    "                        else:\n",
    "                            freq += 1\n",
    "                            word_freq_dict[temp] = freq\n",
    "                    else:\n",
    "                        word_list.append(each_element.lower())\n",
    "\n",
    "    pos = nltk.pos_tag(raw)\n",
    "\n",
    "    for each_word in pos:\n",
    "        wordnet_tag = get_wordnet_pos(each_word[1])\n",
    "\n",
    "        if each_word[1] == \"FW\" or each_word[1] == \"CD\":\n",
    "            w_removed.add(each_word[0].strip())\n",
    "            continue\n",
    "        if len(each_word[0]) == 1 and not (each_word[0] == \"a\" or each_word[0] == \"i\"):\n",
    "            w_removed.add(each_word[0].strip())\n",
    "            continue\n",
    "\n",
    "        word_lemm = lemmatizer.lemmatize(each_word[0], wordnet_tag)\n",
    "\n",
    "        if testing is False:\n",
    "            temp = word_lemm + \"-\" + df.at[index, 'Post Type']\n",
    "            value = word_freq_dict.get(temp)\n",
    "            if value is None:\n",
    "                word_freq_dict[temp] = 1\n",
    "            else:\n",
    "                value += 1\n",
    "                word_freq_dict[temp] = value\n",
    "        else:\n",
    "            if testing:\n",
    "                word_list.append(word_lemm)\n",
    "\n",
    "    pos.clear()\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def train(freq_dict, exp):\n",
    "    word = []\n",
    "    word_list = []\n",
    "    post_type = []\n",
    "    p_ask_hn_dict = {}\n",
    "    p_story_dict = {}\n",
    "    p_show_hn_dict = {}\n",
    "    p_poll_dict = {}\n",
    "    class_probability = []\n",
    "    smoothing = 0.5\n",
    "\n",
    "    if exp == 4:\n",
    "        new_dict = {k: v for k, v in freq_dict.items() if not (v <= remove_freq)}\n",
    "        freq_dict = new_dict\n",
    "    elif exp == 4.5:\n",
    "        sorted_dict_list = sorted(freq_dict.items(), key=operator.itemgetter(1))\n",
    "        remove_elements = int(len(sorted_dict_list) * remove_percent)\n",
    "        new_dict_list = sorted_dict_list[remove_elements:]\n",
    "        freq_dict = dict(new_dict_list)\n",
    "    elif exp == 5:\n",
    "        smoothing = smoothing_value\n",
    "\n",
    "    dict_keys = freq_dict.keys()\n",
    "    freq = list(freq_dict.values())\n",
    "\n",
    "    for each in dict_keys:\n",
    "        word_class = each.split('-')\n",
    "        word.append(word_class[0])\n",
    "        post_type.append(word_class[1])\n",
    "\n",
    "    df = pd.DataFrame({'Word': word, 'Class': post_type, 'Frequency': freq})\n",
    "    df.to_csv(\"vocabulary.csv\")\n",
    "\n",
    "    if df.empty:\n",
    "        experiments.each_accuracy = -1\n",
    "        return\n",
    "\n",
    "    story_df = df[df.Class.str.match('story', case=False)]\n",
    "    ask_hn_df = df[df.Class.str.match('ask_hn', case=False)]\n",
    "    show_hn_df = df[df.Class.str.match('show_hn', case=False)]\n",
    "    poll_df = df[df.Class.str.match('poll', case=False)]\n",
    "\n",
    "    story_dft = df_training[df_training[\"Post Type\"].str.match('story', case=False)]\n",
    "    ask_hn_dft = df_training[df_training[\"Post Type\"].str.match('ask_hn', case=False)]\n",
    "    show_hn_dft = df_training[df_training[\"Post Type\"].str.match('show_hn', case=False)]\n",
    "    poll_dft = df_training[df_training[\"Post Type\"].str.match('poll', case=False)]\n",
    "\n",
    "    show_hn_words = dict(zip(show_hn_df.Word, show_hn_df.Frequency))\n",
    "    ask_hn_words = dict(zip(ask_hn_df.Word, ask_hn_df.Frequency))\n",
    "    poll_words = dict(zip(poll_df.Word, poll_df.Frequency))\n",
    "    story_words = dict(zip(story_df.Word, story_df.Frequency))\n",
    "\n",
    "    show_hn_count = sum(show_hn_words.values())\n",
    "    ask_hn_count = sum(ask_hn_words.values())\n",
    "    poll_count = sum(poll_words.values())\n",
    "    story_count = sum(story_words.values())\n",
    "\n",
    "    vocabulary = df.Word.unique()\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    experiments.no_of_words = vocabulary_size\n",
    "\n",
    "    class_probability_show_hn = len(show_hn_dft.index) / len(df_training.index)\n",
    "    class_probability_ask_hn = len(ask_hn_dft.index) / len(df_training.index)\n",
    "    class_probability_poll = len(poll_dft.index) / len(df_training.index)\n",
    "    class_probability_story = len(story_dft.index) / len(df_training.index)\n",
    "\n",
    "    if smoothing == 0:\n",
    "        vocabulary_size = 0\n",
    "\n",
    "    line_count = 1\n",
    "\n",
    "    for word in vocabulary:\n",
    "        temp_show_hn_freq = show_hn_words[word] if word in show_hn_words else 0\n",
    "        temp_ask_hn_freq = ask_hn_words[word] if word in ask_hn_words else 0\n",
    "        temp_story_freq = story_words[word] if word in story_words else 0\n",
    "        temp_poll_freq = poll_words[word] if word in poll_words else 0\n",
    "\n",
    "        if show_hn_count == 0:\n",
    "            p_word_given_show_hn = 0\n",
    "        else:\n",
    "            p_word_given_show_hn = ((temp_show_hn_freq + smoothing) / (show_hn_count + vocabulary_size))\n",
    "\n",
    "        if ask_hn_count == 0:\n",
    "            p_word_given_ask_hn = 0\n",
    "        else:\n",
    "            p_word_given_ask_hn = ((temp_ask_hn_freq + smoothing) / (ask_hn_count + vocabulary_size))\n",
    "\n",
    "        if poll_count == 0:\n",
    "            p_word_given_poll = 0\n",
    "        else:\n",
    "            p_word_given_poll = ((temp_poll_freq + smoothing) / (poll_count + vocabulary_size))\n",
    "\n",
    "        if story_count == 0:\n",
    "            p_word_given_story = 0\n",
    "        else:\n",
    "            p_word_given_story = ((temp_story_freq + smoothing) / (story_count + vocabulary_size))\n",
    "\n",
    "        if exp == 1:\n",
    "            file = open(\"model-2018.txt\", \"a\")\n",
    "            file.write(str(line_count) + \" \" + str(word) + \" \" + str(temp_story_freq) + \" \" + str(\n",
    "                p_word_given_story) + \" \" + str(\n",
    "                temp_ask_hn_freq) + \" \" + str(p_word_given_ask_hn) + \" \" + str(\n",
    "                temp_show_hn_freq) + \" \" + str(\n",
    "                p_word_given_show_hn) + \" \" + str(temp_poll_freq) + \" \" + str(\n",
    "                p_word_given_poll) + \" \" + '\\n')\n",
    "            file.close()\n",
    "        elif exp == 2:\n",
    "            file = open(\"stopword-model.txt\", \"a\")\n",
    "            file.write(str(line_count) + \" \" + str(word) + \" \" + str(temp_story_freq) + \" \" + str(\n",
    "                p_word_given_story) + \" \" + str(\n",
    "                temp_ask_hn_freq) + \" \" + str(p_word_given_ask_hn) + \" \" + str(\n",
    "                temp_show_hn_freq) + \" \" + str(\n",
    "                p_word_given_show_hn) + \" \" + str(temp_poll_freq) + \" \" + str(\n",
    "                p_word_given_poll) + \" \" + '\\n')\n",
    "            file.close()\n",
    "        elif exp == 3:\n",
    "            file = open(\"wordlength-model.txt\", \"a\")\n",
    "            file.write(str(line_count) + \" \" + str(word) + \" \" + str(temp_story_freq) + \" \" + str(\n",
    "                p_word_given_story) + \" \" + str(\n",
    "                temp_ask_hn_freq) + \" \" + str(p_word_given_ask_hn) + \" \" + str(\n",
    "                temp_show_hn_freq) + \" \" + str(\n",
    "                p_word_given_show_hn) + \" \" + str(temp_poll_freq) + \" \" + str(\n",
    "                p_word_given_poll) + \" \" + '\\n')\n",
    "            file.close()\n",
    "        line_count += 1\n",
    "\n",
    "        p_ask_hn_dict[word] = p_word_given_ask_hn\n",
    "        p_show_hn_dict[word] = p_word_given_show_hn\n",
    "        p_poll_dict[word] = p_word_given_poll\n",
    "        p_story_dict[word] = p_word_given_story\n",
    "        word_list.append(word)\n",
    "\n",
    "    end_time = time.process_time() - start_time\n",
    "    print(\"\\nTime to train:\", end_time)\n",
    "\n",
    "    # 0: show_hn\n",
    "    # 1: ask_hn\n",
    "    # 2: poll\n",
    "    # 3: story\n",
    "\n",
    "    class_probability.append(class_probability_show_hn)\n",
    "    class_probability.append(class_probability_ask_hn)\n",
    "    class_probability.append(class_probability_poll)\n",
    "    class_probability.append(class_probability_story)\n",
    "\n",
    "    accuracy = experiments.baseline(class_probability, df_testing, p_show_hn_dict, p_ask_hn_dict, p_poll_dict,\n",
    "                                    p_story_dict, exp)\n",
    "\n",
    "    if exp == 4 or exp == 4.5 or exp == 5:\n",
    "        experiments.each_accuracy = accuracy\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return nltk.wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return nltk.wordnet.NOUN\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    read_file()\n",
    "    experiments.select_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
